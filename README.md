<p align="center">
  <h1 align="center">ğŸ™ï¸ Talk2Scene</h1>
  <p align="center">
    <em>Audio-driven intelligent animation generation â€” from dialogue to visual storytelling.</em>
  </p>
  <p align="center">
    <img src="https://img.shields.io/badge/python-3.11%2B-blue?logo=python&logoColor=white" alt="Python 3.11+">
    <img src="https://img.shields.io/badge/license-Apache%202.0-green" alt="License">
    <img src="https://img.shields.io/badge/package_manager-uv-blueviolet?logo=uv" alt="uv">
    <img src="https://img.shields.io/badge/config-Hydra-orange?logo=meta" alt="Hydra">
    <img src="https://img.shields.io/badge/LLM-GPT--4o-black?logo=openai" alt="GPT-4o">
  </p>
</p>

---

Talk2Scene is an **audio-driven intelligent animation tool** that automatically parses voice dialogue files, recognizes text content and timestamps, and uses AI to recommend matching **character stances (STA)**, **expressions (EXP)**, **actions (ACT)**, **backgrounds (BG)**, and **CG illustrations** inserted at the right moments. It produces structured scene event data and composes preview videos showing AI characters performing dynamically across scenes.

Designed for **content creators**, **educators**, **virtual streamers**, and **AI enthusiasts** â€” Talk2Scene turns audio into engaging visual narratives for interview videos, AI interactive demos, educational presentations, and more.

## ğŸ’¡ Why Talk2Scene

Manually composing visual scenes for dialogue-driven content is tedious and error-prone. Talk2Scene automates the entire workflow: feed in audio or a transcript, and the pipeline produces **time-synced scene events** â€” ready for browser playback or video export â€” without touching a single frame by hand.

## ğŸ—ï¸ Architecture

```
Audio / Transcript
        â”‚
   Transcription (Whisper / OpenAI API)
        â”‚
   Scene Generation (LLM)
        â”‚
   JSONL Events â”€â”€â¤ Browser Viewer (web/)
        â”‚               Static PNG Render
        â”‚               Video Export (ffmpeg)
        â–¼
   Session Output
```

Scenes are composed from **five layer types** stacked bottom-up:

> **BG** â†’ **STA** â†’ **ACT** â†’ **EXP**
>
> A **CG** illustration, when active, replaces the entire layered scene.

## ğŸ“¦ Install

> [!IMPORTANT]
> Requires **Python 3.11+**, [uv](https://docs.astral.sh/uv/), and **FFmpeg**.

```bash
uv sync
```

Set your OpenAI API key:

```bash
export OPENAI_API_KEY="your-key"
```

## ğŸš€ Usage

```bash
uv run talk2scene --help
```

### ğŸ“ Text Mode

Generate scenes from a pre-transcribed JSONL file:

```bash
uv run talk2scene mode=text io.input.text_file=path/to/transcript.jsonl
```

### ğŸ§ Batch Mode

Process an audio file end-to-end (place audio in `input/`):

```bash
uv run talk2scene mode=batch
```

### ğŸ¬ Video Mode

Render a completed session into video:

```bash
uv run talk2scene mode=video session_id=SESSION_ID
```

### ğŸ“¡ Stream Mode

Consume audio or pre-transcribed text from Redis in real time:

```bash
uv run talk2scene mode=stream
```

## ğŸ“š Documentation

Full documentation is available in `docs/` (English & ä¸­æ–‡). Serve locally:

```bash
uv sync --extra docs && uv run mkdocs serve
```

| | Topic | Description |
|---|-------|-------------|
| ğŸ”§ | Installation | Prerequisites and setup |
| âŒ¨ï¸ | CLI Usage | All modes and overrides |
| âš™ï¸ | Configuration | Hydra config groups |
| ğŸ“¡ | Redis Streaming | Real-time dual-stream setup |
| ğŸ“„ | JSONL Schema | Event types and format |
| ğŸ–¥ï¸ | Frontend | Browser viewer and playback |
| ğŸ¨ | Assets | Layer assets and placeholder generator |
| ğŸ–¼ï¸ | Scene Renderer | Composition and rendering |
| âœ… | Evaluation | Visual regression testing |
| ğŸ“‹ | Whitelist | Valid component codes |

## ğŸ“¬ Contact

- âœ‰ï¸ Email: **hobart.yang@qq.com**
- ğŸ› Issues: [Open an issue](../../issues) on GitHub

## ğŸ“„ License

Licensed under the [Apache License 2.0](LICENSE).
