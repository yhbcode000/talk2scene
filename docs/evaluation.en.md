# ğŸ§ª Evaluation Framework

The evaluation framework is **separate from unit tests**. It performs visual regression testing by rendering scenes and comparing with golden PNGs.

## ğŸ“ Structure

```
evaluation/
â”œâ”€â”€ cases/      # Scene input JSON files
â”œâ”€â”€ expected/   # Golden PNG images
â”œâ”€â”€ output/     # Rendered PNGs (generated)
â””â”€â”€ diffs/      # Diff images on failure
```

## ğŸš€ Running Evaluation

```bash
uv run talk2scene eval.run=true
```

## ğŸ” How It Works

```mermaid
flowchart TD
    A[evaluation/cases/*.json] --> B[Render scene to PNG]
    B --> C{Compare with\nexpected PNG}
    C -->|within tolerance| D[Pass]
    C -->|exceeds tolerance| E[Write diff image]
    D --> F[JSON report\n+ text summary]
    E --> F
```

## ğŸ“ Comparison Methods

- ğŸ‘ï¸ **Pixel diff**: Percentage of differing pixels (configurable tolerance)
- #ï¸âƒ£ **Perceptual hash**: Hamming distance between image hashes

## ğŸ†š Tests vs Evaluation

| | tests/ | evaluation/ |
|---|--------|-------------|
| ğŸ·ï¸ Type | Unit tests | Visual regression |
| ğŸ› ï¸ Tool | pytest | Built-in runner |
| âœ… Checks | Logic correctness | Render correctness |
| ğŸ“¦ Artifacts | - | PNG renders + diffs |
